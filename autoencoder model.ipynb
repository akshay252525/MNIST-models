{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962271e5-8d86-46af-a91b-833fc57fffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84c5f1-6483-4d5b-8812-54cdae44de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file, delimiter=\",\", header = None)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.df.iloc[index]\n",
    "        image_in_1D = torch.from_numpy(entry[1:].to_numpy())\n",
    "        image_in_3D = torch.reshape(image_in_1D, (1, 28, 28))\n",
    "        image = image_in_3D / 255\n",
    "        label = torch.tensor(entry[0])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3063d5-5848-45d6-8511-fa6c4e7e4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(\"mnist_train.csv\")\n",
    "test_dataset = MyDataset(\"mnist_test.csv\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 100, shuffle = True) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 100, shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13d91ed-56d1-4282-997a-0519426aca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is almost exactly the class 'MyModel' from classification pl model.ipynb\n",
    "# Removed pooling layer since unpooling seems unsatisfactory\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 28, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(28, 28, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        # self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(21952, 512)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input 1x28x28, output 28x28x28\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.drop1(x)     \n",
    "        \n",
    "        # input 28x28x28, output 28x28x28\n",
    "        x = self.act2(self.conv2(x))\n",
    "        \n",
    "        # input 28x28x28, output 21952\n",
    "        x = self.flat(x)\n",
    "        \n",
    "        # input 5488, output 512\n",
    "        x = self.act3(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        # input 512, output 10\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19a5c7-5be4-4d5c-8a32-10f15f5e1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undoing everything done above\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(10, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 21952)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.unflat = nn.Unflatten(1,(28,28,28))\n",
    "        \n",
    "        self.deconv3 = nn.ConvTranspose2d(28, 28, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(28, 1, kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # input 10, output 512\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        # input 512, output 21952\n",
    "        x = self.act2(self.fc2(x))\n",
    "\n",
    "        # input 21952, output 28x28x28\n",
    "        x = self.unflat(x)\n",
    "        \n",
    "        # input 28x28x28, output 28x28x28\n",
    "        x = self.act3(self.deconv3(x))\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        # input 28x28x28, output 1x28x28\n",
    "        x = self.deconv4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3aef1-673a-4bd7-a4bc-f660594e4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/08-deep-autoencoders.html\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderModel()\n",
    "        self.decoder = DecoderModel()\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, _ = batch\n",
    "        autoencoded_images = self(images)\n",
    "        loss = self.loss_fn(autoencoded_images, images)\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss   \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, _ = batch\n",
    "        autoencoded_images = self(images)\n",
    "        loss = self.loss_fn(autoencoded_images, images)\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31161f5d-c2a2-4e42-a6bf-c6c3cd589bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder.load_from_checkpoint(\"lightning_logs/version_24/checkpoints/epoch=4-step=3000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e27a8-61cd-4107-9693-090afaa1ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=9Vc7tTWZark&list=PLaMu-SDt_RB6b4Z_kOUAlT0KI6jTMCKPL&index=2\n",
    "    \n",
    "# callbacks = [ModelCheckpoint(save_top_k=-1, mode=\"min\", monitor=\"val_loss\")]\n",
    "# lightning_model = Autoencoder()\n",
    "# trainer = pl.Trainer(max_epochs=5, callbacks=callbacks)\n",
    "# trainer.fit(model=lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc43efb-b6d5-4318-ae67-ccf94614c58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy, loss curves from logged data\n",
    "\n",
    "metrics = pd.read_csv(\"lightning_logs/version_24/metrics.csv\")\n",
    "\n",
    "train_loss = np.zeros(5)\n",
    "val_loss = np.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    train_loss[i] = metrics.iloc[13+(14*i)].iloc[2].item()\n",
    "    val_loss[i] = metrics.iloc[12+(14*i)].iloc[-1].item()\n",
    "\n",
    "print(train_loss)\n",
    "print(val_loss)\n",
    "\n",
    "epochs = np.array([1,2,3,4,5])\n",
    "\n",
    "plt.plot(epochs, train_loss, label=\"train_loss\")\n",
    "plt.plot(epochs, val_loss, label=\"val_loss\")\n",
    "plt.title(\"Training vs validation loss during first 5 epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4197ffe-ad6c-4e68-b22e-322112299c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the results\n",
    "\n",
    "samples = random.sample(list(range(0, 60000)), 10)\n",
    "\n",
    "image_label = []\n",
    "autoencoded_loss = np.zeros(10)\n",
    "counter = 0\n",
    "\n",
    "# data = [['MNIST image number', 'Digit represented', 'Average MSE loss for this image'],]\n",
    "\n",
    "for i in samples:    \n",
    "    image, label = train_dataset[i]\n",
    "    image_label.append(label.item())\n",
    "    image = torch.reshape(image, (28, 28))\n",
    "    plt.imshow(image, cmap=\"grey\")\n",
    "    # plt.savefig(f\"MNIST_image_number_{i}.png\")\n",
    "    plt.title(f\"Real image number {i}\")\n",
    "    plt.show()\n",
    "    image_hat = torch.reshape(image, (1, 1, 28, 28))\n",
    "    autoencoded_image = lightning_model(image_hat)\n",
    "    autoencoded_image = torch.reshape(autoencoded_image, (28, 28))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(autoencoded_image, image)\n",
    "    autoencoded_loss[counter] = loss\n",
    "    plt.imshow(autoencoded_image.detach().numpy(), cmap=\"grey\")\n",
    "    # plt.savefig(f\"Autoencoded_MNIST_image_number_{i}.png\")\n",
    "    plt.title(f\"Autoencoded image number {i}\")\n",
    "    plt.show()\n",
    "    print(f\"MSE Loss averaged over image = {loss}\")\n",
    "    # entry = [i, label, loss]\n",
    "    # data.append(entry)\n",
    "    counter += 1\n",
    "\n",
    "# csv_file_path = 'testing_autoencoder_on_MNIST.csv'\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eba5fa2-c192-4481-9e14-debabe56ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"What pictures contained: {image_label}\")\n",
    "print(f\"MSE loss: {np.round(autoencoded_loss, 4)}\")\n",
    "print(f\"Overall mean loss: {np.round(np.mean(autoencoded_loss), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38d86e-2217-44d5-86e4-fe3b90742d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing to non-digit test images\n",
    "\n",
    "test_image_label = [\"A\", \"C\", \"E\", \"H\", \"L\", \"Q\", \"W\", \"Y\", \"%\", \"&\"]\n",
    "test_autoencoded_loss = np.zeros(10)\n",
    "counter = 0\n",
    "\n",
    "# new_data = data = [['Letter', 'Average MSE loss for this image'],]\n",
    "\n",
    "for i in test_image_label:\n",
    "    image = Image.open(f\"test_images/{i}.png\").convert('L')\n",
    "    image = np.array(image)\n",
    "    plt.imshow(image, cmap=\"grey\")\n",
    "    # plt.savefig(f\"Original_image_of_{i}\")\n",
    "    plt.title(f\"Original image of {i}\")\n",
    "    plt.show()\n",
    "    image = torch.tensor(image).float()\n",
    "    image_hat = torch.reshape(image, (1, 1, 28, 28))\n",
    "    autoencoded_image = lightning_model(image_hat)\n",
    "    autoencoded_image = torch.reshape(autoencoded_image, (28, 28))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(autoencoded_image, image)\n",
    "    test_autoencoded_loss[counter] = loss\n",
    "    plt.imshow(autoencoded_image.detach().numpy(), cmap=\"grey\")\n",
    "    # plt.savefig(f\"Autoencoded_image_of_{i}\")\n",
    "    plt.title(f\"Autoencoded image of {i}\")\n",
    "    plt.show()\n",
    "    print(f\"MSE Loss averaged over image = {loss}\")\n",
    "    # entry = [i, loss]\n",
    "    # new_data.append(entry)\n",
    "    counter += 1\n",
    "\n",
    "# csv_file_path = 'testing_autoencoder_on_letters.csv'\n",
    "# new_df = pd.DataFrame(new_data)\n",
    "# new_df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468d252-3658-48fc-9080-0ebdd51d3dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"What pictures contained: {test_image_label}\")\n",
    "print(f\"MSE loss > {np.floor(test_autoencoded_loss)}\")\n",
    "print(f\"Overall mean loss > {np.floor(np.mean(test_autoencoded_loss))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f33068-5356-4b67-ae79-b552c5877779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicekernel",
   "language": "python",
   "name": "practicekernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
