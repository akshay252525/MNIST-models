{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36522119-4d75-49ab-9bf3-f453757f457a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import matplotlib.image as img\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2dfad-ef00-4837-ac80-7b75359e3903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file, delimiter=\",\", header = None)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        entry = self.df.iloc[index]\n",
    "        image_in_1D = torch.from_numpy(entry[1:].to_numpy())\n",
    "        image_in_3D = torch.reshape(image_in_1D, (1, 28, 28))\n",
    "        image = image_in_3D / 255\n",
    "        label = torch.tensor(entry[0])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e3f12-08c7-459c-aebf-a7d4681bd801",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(\"mnist_train.csv\")\n",
    "test_dataset = MyDataset(\"mnist_test.csv\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 100, shuffle = True) \n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 100, shuffle = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f03fb9-a147-49b1-883e-36acac118890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is almost exactly the class 'EncoderModel' from autoencoder model.ipynb\n",
    "# With the addition of a pooling layer\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 28, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(28, 28, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), return_indices=True)\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc3 = nn.Linear(5488, 512)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc4 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input 1x28x28, output 28x28x28\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.drop1(x)     \n",
    "        \n",
    "        # input 28x28x28, output 28x14x14\n",
    "        x = self.act2(self.conv2(x))\n",
    "        x, indices = self.pool2(x)\n",
    "        \n",
    "        # input 28x14x14, output 5488\n",
    "        x = self.flat(x)\n",
    "        \n",
    "        # input 5488, output 512\n",
    "        x = self.act3(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        # input 512, output 10\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e259f78-9b78-418e-9a2d-744da362788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undoing everything done above\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(10, 512)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 5488)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.unflat = nn.Unflatten(1,(28,14,14))\n",
    "\n",
    "        self.unpool3 = nn.MaxUnpool2d(kernel_size=(2,2), stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(28, 28, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "\n",
    "        self.deconv4 = nn.ConvTranspose2d(28, 1, kernel_size=(3,3), stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x, indices):\n",
    "\n",
    "        # input 10, output 512\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        # input 512, output 5488\n",
    "        x = self.act2(self.fc2(x))\n",
    "\n",
    "        # input 5488, output 28x14x14\n",
    "        x = self.unflat(x)\n",
    "        \n",
    "        # input 28x14x14, output 28x28x28\n",
    "        x = self.unpool3(x, indices)\n",
    "        x = self.act3(self.deconv3(x))\n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        # input 28x28x28, output 1x28x28\n",
    "        x = self.deconv4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df622659-65e8-4dde-b9e6-80370aeaef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/08-deep-autoencoders.html\n",
    "\n",
    "class Autoencoder(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderModel()\n",
    "        self.decoder = DecoderModel()\n",
    "        self.loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z, indices = self.encoder(x)\n",
    "        x_hat = self.decoder(z, indices)\n",
    "        return x_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, _ = batch\n",
    "        autoencoded_images = self(images)\n",
    "        loss = self.loss_fn(autoencoded_images, images)\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss   \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, _ = batch\n",
    "        autoencoded_images = self(images)\n",
    "        loss = self.loss_fn(autoencoded_images, images)\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        self.log(\"val_loss\", loss, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3287995f-72a8-43b9-9f02-7ac17a831791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=9Vc7tTWZark&list=PLaMu-SDt_RB6b4Z_kOUAlT0KI6jTMCKPL&index=2\n",
    "    \n",
    "# callbacks = [ModelCheckpoint(save_top_k=-1, mode=\"min\", monitor=\"val_loss\")]\n",
    "# lightning_model = Autoencoder()\n",
    "# trainer = pl.Trainer(max_epochs=5, callbacks=callbacks)\n",
    "# trainer.fit(model=lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491c27a-5851-4a4d-a7e2-927703c9326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = Autoencoder.load_from_checkpoint(\"lightning_logs/version_25/checkpoints/epoch=4-step=3000.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9516a8-0015-479d-927d-d8e64049748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting accuracy, loss curves from logged data\n",
    "\n",
    "metrics = pd.read_csv(\"lightning_logs/version_25/metrics.csv\")\n",
    "\n",
    "train_loss = np.zeros(5)\n",
    "val_loss = np.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    train_loss[i] = metrics.iloc[13+(14*i)].iloc[2].item()\n",
    "    val_loss[i] = metrics.iloc[12+(14*i)].iloc[-1].item()\n",
    "\n",
    "print(train_loss)\n",
    "print(val_loss)\n",
    "\n",
    "epochs = np.array([1,2,3,4,5])\n",
    "\n",
    "plt.plot(epochs, train_loss, label=\"train_loss\")\n",
    "plt.plot(epochs, val_loss, label=\"val_loss\")\n",
    "plt.title(\"Training vs validation loss during first 5 epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f735d5-bd0d-4814-a71a-e503c06671e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the results\n",
    "\n",
    "samples = random.sample(list(range(0, 60000)), 10)\n",
    "\n",
    "image_label = []\n",
    "autoencoded_loss = np.zeros(10)\n",
    "counter = 0\n",
    "\n",
    "# data = [['MNIST image number ', 'Digit represented ', 'Average MSE loss for this image'],]\n",
    "\n",
    "for i in samples:    \n",
    "    image, label = train_dataset[i]\n",
    "    image_label.append(label.item())\n",
    "    image = torch.reshape(image, (28, 28))\n",
    "    plt.imshow(image, cmap=\"grey\")\n",
    "    # plt.savefig(f\"MNIST_image_number_{i}.png\")\n",
    "    plt.title(f\"Real image number {i}\")\n",
    "    plt.show()\n",
    "    image_hat = torch.reshape(image, (1, 1, 28, 28))\n",
    "    autoencoded_image = lightning_model(image_hat)\n",
    "    autoencoded_image = torch.reshape(autoencoded_image, (28, 28))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(autoencoded_image, image)\n",
    "    autoencoded_loss[counter] = loss\n",
    "    plt.imshow(autoencoded_image.detach().numpy(), cmap=\"grey\")\n",
    "    # plt.savefig(f\"Autoencoded_MNIST_image_number_{i}.png\")\n",
    "    plt.title(f\"Autoencoded image number {i}\")\n",
    "    plt.show()\n",
    "    print(f\"MSE Loss averaged over image = {loss}\")\n",
    "    # entry = [f\"{i} \",f\"{label} \",f\"{loss.item()}\"]\n",
    "    # data.append(entry)\n",
    "    counter += 1\n",
    "\n",
    "# csv_file_path = 'testing_autoencoder_with_pooling_on_MNIST.csv'\n",
    "# df = pd.DataFrame(data)\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7d07a-49fd-4f6f-b974-08f171b4d2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"What pictures contained: {image_label}\")\n",
    "print(f\"MSE loss: {np.round(autoencoded_loss, 4)}\")\n",
    "print(f\"Overall mean loss: {np.round(np.mean(autoencoded_loss), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c115b0-9dba-48c1-9dd3-53b46d8c4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing to non-digit test images\n",
    "\n",
    "test_image_label = [\"A\", \"C\", \"E\", \"H\", \"L\", \"Q\", \"W\", \"Y\", \"%\", \"&\"]\n",
    "test_autoencoded_loss = np.zeros(10)\n",
    "counter = 0\n",
    "\n",
    "# new_data = [['Letter ',' Autoencoder MSE loss'],]\n",
    "\n",
    "for i in test_image_label:\n",
    "    image = Image.open(f\"test_images/{i}.png\").convert('L')\n",
    "    image = np.array(image)\n",
    "    plt.imshow(image, cmap=\"grey\")\n",
    "    # plt.savefig(f\"Original_image_of_{i}.png\")\n",
    "    plt.title(f\"Original image of {i}\")\n",
    "    plt.show()\n",
    "    image = torch.tensor(image).float()\n",
    "    image_hat = torch.reshape(image, (1, 1, 28, 28))\n",
    "    autoencoded_image = lightning_model(image_hat)\n",
    "    autoencoded_image = torch.reshape(autoencoded_image, (28, 28))\n",
    "    loss_fn = nn.MSELoss()\n",
    "    loss = loss_fn(autoencoded_image, image)\n",
    "    test_autoencoded_loss[counter] = loss\n",
    "    plt.imshow(autoencoded_image.detach().numpy(), cmap=\"grey\")\n",
    "    # plt.savefig(f\"Autoencoded_image_of_{i}.png\")\n",
    "    plt.title(f\"Autoencoded image of {i}\")\n",
    "    plt.show()\n",
    "    print(f\"MSE Loss averaged over image = {loss}\")\n",
    "    # entry = [f\"{i} \",f\" {loss}\"]\n",
    "    # new_data.append(entry)\n",
    "    counter += 1\n",
    "\n",
    "# csv_file_path = 'testing_autoencoder_with_pooling_on_letters.csv'\n",
    "# new_df = pd.DataFrame(new_data)\n",
    "# df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ae777e-8de7-4ea3-97a2-95e3aa33e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"What pictures contained: {test_image_label}\")\n",
    "print(f\"MSE loss > {np.floor(test_autoencoded_loss)}\")\n",
    "print(f\"Overall mean loss > {np.floor(np.mean(test_autoencoded_loss))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a816975-5180-4eb9-b2c7-50d3e1022b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicekernel",
   "language": "python",
   "name": "practicekernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
